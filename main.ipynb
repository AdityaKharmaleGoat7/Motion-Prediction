{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e870cef9-774f-4a15-ad98-9fed6e024e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12628/1239395359.py:47: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cv.circle(frame, (int(predicted_coords[0]), int(predicted_coords[1])), 20, [0, 255, 255], 2, 8)\n",
      "/tmp/ipykernel_12628/1239395359.py:48: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cv.putText(frame, \"Predicted\", (int(predicted_coords[0] + 50), int(predicted_coords[1] + 20)), cv.FONT_HERSHEY_SIMPLEX, 0.5, [0, 255, 255], 2)\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     cv\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgram Completed!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m humans \u001b[38;5;241m=\u001b[39m \u001b[43mprocessImg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDetectHumans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m frame \u001b[38;5;241m=\u001b[39m processImg\u001b[38;5;241m.\u001b[39mPredictMotion(frame, humans)\n\u001b[1;32m     66\u001b[0m cv\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuman Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mProcessImage.DetectHumans\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     gray \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m---> 31\u001b[0m     humans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaleFactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminNeighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m humans\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "MAX_HUMANS_TO_TRACK = 10\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self):\n",
    "        self.kf = cv.KalmanFilter(4, 2)\n",
    "        self.kf.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n",
    "        self.kf.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
    "\n",
    "    def Estimate(self, coordX, coordY):\n",
    "        measured = np.array([[np.float32(coordX)], [np.float32(coordY)]])\n",
    "        predicted = self.kf.predict()\n",
    "        self.kf.correct(measured)\n",
    "        return predicted\n",
    "\n",
    "class ProcessImage:\n",
    "    def __init__(self):\n",
    "        self.human_cascade = cv.CascadeClassifier('haarcascade_fullbody.xml') # Path to Haar cascade for human detection\n",
    "        self.kfObjs = []\n",
    "        for _ in range(MAX_HUMANS_TO_TRACK):\n",
    "            self.kfObjs.append(KalmanFilter())\n",
    "        self.frame_count = 0\n",
    "        self.detector_frequency = 1  # Perform detection every 5 frames\n",
    "\n",
    "    def DetectHumans(self, frame):\n",
    "        self.frame_count += 1\n",
    "        if self.frame_count % self.detector_frequency == 0:\n",
    "            gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "            humans = self.human_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "            return humans\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def PredictMotion(self, frame, humans):\n",
    "        for i, human in enumerate(humans):\n",
    "            if i >= MAX_HUMANS_TO_TRACK:\n",
    "                break\n",
    "            x, y, w, h = human\n",
    "            human_center = (x + w // 2, y + h // 2)\n",
    "            predicted_coords = self.kfObjs[i].Estimate(*human_center)\n",
    "            # Draw the actual position\n",
    "            cv.circle(frame, human_center, 20, [0, 0, 255], 2, 8)\n",
    "            cv.putText(frame, \"Actual\", (human_center[0] + 50, human_center[1] + 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, [0, 0, 255], 2)\n",
    "            # Draw the predicted position\n",
    "            cv.circle(frame, (int(predicted_coords[0]), int(predicted_coords[1])), 20, [0, 255, 255], 2, 8)\n",
    "            cv.putText(frame, \"Predicted\", (int(predicted_coords[0] + 50), int(predicted_coords[1] + 20)), cv.FONT_HERSHEY_SIMPLEX, 0.5, [0, 255, 255], 2)\n",
    "        return frame\n",
    "\n",
    "def main():\n",
    "    processImg = ProcessImage()\n",
    "    vid = cv.VideoCapture('green.mp4')\n",
    "    if not vid.isOpened():\n",
    "        print('Cannot open input video')\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        humans = processImg.DetectHumans(frame)\n",
    "        frame = processImg.PredictMotion(frame, humans)\n",
    "\n",
    "        cv.imshow('Human Detection', frame)\n",
    "        if cv.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    vid.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print('Program Completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b532fd-939d-4b16-a049-915353006d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "MAX_HUMANS_TO_TRACK = 20\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self):\n",
    "        self.kf = cv.KalmanFilter(4, 2)\n",
    "        self.kf.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n",
    "        self.kf.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
    "\n",
    "    def Estimate(self, coordX, coordY):\n",
    "        measured = np.array([[np.float32(coordX)], [np.float32(coordY)]])\n",
    "        predicted = self.kf.predict()\n",
    "        self.kf.correct(measured)\n",
    "        return predicted\n",
    "\n",
    "class ProcessImage:\n",
    "    def __init__(self):\n",
    "        self.net = cv.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')  # Load YOLOv3 model\n",
    "        self.output_layers = self.net.getUnconnectedOutLayersNames()\n",
    "        self.kfObjs = []\n",
    "        for _ in range(MAX_HUMANS_TO_TRACK):\n",
    "            self.kfObjs.append(KalmanFilter())\n",
    "        self.frame_count = 0\n",
    "        self.detector_frequency = 1  # Perform detection every frame\n",
    "\n",
    "    def DetectHumans(self, frame):\n",
    "        self.frame_count += 1\n",
    "        blob = cv.dnn.blobFromImage(frame, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        outs = self.net.forward(self.output_layers)\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5 and class_id == 0:  # Class ID for person in COCO dataset\n",
    "                    center_x = int(detection[0] * frame.shape[1])\n",
    "                    center_y = int(detection[1] * frame.shape[0])\n",
    "                    w = int(detection[2] * frame.shape[1])\n",
    "                    h = int(detection[3] * frame.shape[0])\n",
    "                    x = center_x - w // 2\n",
    "                    y = center_y - h // 2\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "        indexes = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        humans = []\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                humans.append((x, y, w, h))\n",
    "        return humans\n",
    "\n",
    "    def PredictMotion(self, frame, humans):\n",
    "        for i, human in enumerate(humans):\n",
    "            if i >= MAX_HUMANS_TO_TRACK:\n",
    "                break\n",
    "            x, y, w, h = human\n",
    "            human_center = (x + w // 2, y + h // 2)\n",
    "            predicted_coords = self.kfObjs[i].Estimate(*human_center)\n",
    "            # Draw the actual position\n",
    "            cv.circle(frame, human_center, 20, [0, 0, 255], 2, 8)\n",
    "            cv.putText(frame, \"Actual\", (human_center[0] + 50, human_center[1] + 20), cv.FONT_HERSHEY_SIMPLEX, 0.5, [0, 0, 255], 2)\n",
    "            # Draw the predicted position\n",
    "            cv.circle(frame, (int(predicted_coords[0]), int(predicted_coords[1])), 20, [0, 255, 255], 2, 8)\n",
    "            cv.putText(frame, \"Predicted\", (int(predicted_coords[0] + 50), int(predicted_coords[1] + 20)), cv.FONT_HERSHEY_SIMPLEX, 0.5, [0, 255, 255], 2)\n",
    "        return frame\n",
    "\n",
    "def main():\n",
    "    processImg = ProcessImage()\n",
    "    vid = cv.VideoCapture('crowd.mp4')\n",
    "    if not vid.isOpened():\n",
    "        print('Cannot open input video')\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        humans = processImg.DetectHumans(frame)\n",
    "        frame = processImg.PredictMotion(frame, humans)\n",
    "\n",
    "        cv.imshow('Human Detection', frame)\n",
    "        if cv.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    vid.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print('Program Completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200cb91a-a496-4510-ba8b-85a2aeff31ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n",
      "/tmp/ipykernel_12748/3407192093.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  predicted_center = (int(predicted_coords[0]), int(predicted_coords[1]))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgram Completed!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 89\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m humans \u001b[38;5;241m=\u001b[39m \u001b[43mprocessImg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDetectHumans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m frame \u001b[38;5;241m=\u001b[39m processImg\u001b[38;5;241m.\u001b[39mPredictMotion(frame, humans)\n\u001b[1;32m     92\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuman Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mProcessImage.DetectHumans\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     31\u001b[0m blob \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mblobFromImage(frame, \u001b[38;5;241m0.00392\u001b[39m, (\u001b[38;5;241m416\u001b[39m, \u001b[38;5;241m416\u001b[39m), swapRB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39msetInput(blob)\n\u001b[0;32m---> 33\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m class_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     35\u001b[0m confidences \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "MAX_HUMANS_TO_TRACK = 20\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self):\n",
    "        self.kf = cv2.KalmanFilter(4, 2)\n",
    "        self.kf.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n",
    "        self.kf.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
    "\n",
    "    def Estimate(self, coordX, coordY):\n",
    "        measured = np.array([[np.float32(coordX)], [np.float32(coordY)]])\n",
    "        predicted = self.kf.predict()\n",
    "        self.kf.correct(measured)\n",
    "        return predicted\n",
    "\n",
    "class ProcessImage:\n",
    "    def __init__(self):\n",
    "        self.net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')  # Load YOLOv3 model\n",
    "        self.output_layers = self.net.getUnconnectedOutLayersNames()\n",
    "        self.kfObjs = []\n",
    "        self.prev_centers = {}\n",
    "        for _ in range(MAX_HUMANS_TO_TRACK):\n",
    "            self.kfObjs.append(KalmanFilter())\n",
    "        self.frame_count = 0\n",
    "        self.detector_frequency = 1  # Perform detection every frame\n",
    "\n",
    "    def DetectHumans(self, frame):\n",
    "        self.frame_count += 1\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        outs = self.net.forward(self.output_layers)\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5 and class_id == 0:  # Class ID for person in COCO dataset\n",
    "                    center_x = int(detection[0] * frame.shape[1])\n",
    "                    center_y = int(detection[1] * frame.shape[0])\n",
    "                    w = int(detection[2] * frame.shape[1])\n",
    "                    h = int(detection[3] * frame.shape[0])\n",
    "                    x = center_x - w // 2\n",
    "                    y = center_y - h // 2\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        humans = []\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                center = (x + w // 2, y + h // 2)\n",
    "                humans.append((center, (x, y, w, h)))\n",
    "        return humans\n",
    "\n",
    "    def PredictMotion(self, frame, humans):\n",
    "        for i, (center, box) in enumerate(humans):\n",
    "            if i >= MAX_HUMANS_TO_TRACK:\n",
    "                break\n",
    "            prev_center = self.prev_centers.get(i)\n",
    "            if prev_center:\n",
    "                prev_center = tuple(map(int, prev_center))\n",
    "                predicted_coords = self.kfObjs[i].Estimate(*prev_center)\n",
    "                predicted_center = (int(predicted_coords[0]), int(predicted_coords[1]))\n",
    "                cv2.circle(frame, predicted_center, 20, [0, 255, 255], 2, 8)\n",
    "                cv2.putText(frame, \"Predicted\", (predicted_center[0] + 50, predicted_center[1] + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0, 255, 255], 2)\n",
    "            self.prev_centers[i] = center\n",
    "            cv2.circle(frame, center, 20, [0, 0, 255], 2, 8)\n",
    "            cv2.putText(frame, \"Actual\", (center[0] + 50, center[1] + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0, 0, 255], 2)\n",
    "        return frame\n",
    "\n",
    "def main():\n",
    "    processImg = ProcessImage()\n",
    "    vid = cv2.VideoCapture('fast.mp4')\n",
    "    if not vid.isOpened():\n",
    "        print('Cannot open input video')\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        humans = processImg.DetectHumans(frame)\n",
    "        frame = processImg.PredictMotion(frame, humans)\n",
    "\n",
    "        cv2.imshow('Human Detection', frame)\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print('Program Completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697d240-0cab-4379-a55a-cb1377a0a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "MAX_HUMANS_TO_TRACK = 20\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self):\n",
    "        self.kf = cv2.KalmanFilter(4, 2)\n",
    "        self.kf.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\n",
    "        self.kf.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
    "\n",
    "    def Estimate(self, coordX, coordY):\n",
    "        measured = np.array([[np.float32(coordX)], [np.float32(coordY)]])\n",
    "        predicted = self.kf.predict()\n",
    "        self.kf.correct(measured)\n",
    "        return predicted\n",
    "\n",
    "class ProcessImage:\n",
    "    def __init__(self):\n",
    "        self.net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')  # Load YOLOv3 model\n",
    "        self.output_layers = self.net.getUnconnectedOutLayersNames()\n",
    "        self.kfObjs = []\n",
    "        self.humans = []\n",
    "        self.prev_centers = {}\n",
    "        for _ in range(MAX_HUMANS_TO_TRACK):\n",
    "            self.kfObjs.append(KalmanFilter())\n",
    "        self.frame_count = 0\n",
    "        self.detector_frequency = 30  # Perform detection every 30 frames\n",
    "        self.detected_once = False\n",
    "\n",
    "    def DetectHumans(self, frame):\n",
    "        if not self.detected_once or self.frame_count % self.detector_frequency == 0:\n",
    "            blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "            self.net.setInput(blob)\n",
    "            outs = self.net.forward(self.output_layers)\n",
    "            class_ids = []\n",
    "            confidences = []\n",
    "            boxes = []\n",
    "            for out in outs:\n",
    "                for detection in out:\n",
    "                    scores = detection[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "                    if confidence > 0.5 and class_id == 0:  # Class ID for person in COCO dataset\n",
    "                        center_x = int(detection[0] * frame.shape[1])\n",
    "                        center_y = int(detection[1] * frame.shape[0])\n",
    "                        w = int(detection[2] * frame.shape[1])\n",
    "                        h = int(detection[3] * frame.shape[0])\n",
    "                        x = center_x - w // 2\n",
    "                        y = center_y - h // 2\n",
    "                        boxes.append([x, y, w, h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        class_ids.append(class_id)\n",
    "            indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "            self.humans = []\n",
    "            for i in range(len(boxes)):\n",
    "                if i in indexes:\n",
    "                    x, y, w, h = boxes[i]\n",
    "                    center = (x + w // 2, y + h // 2)\n",
    "                    self.humans.append((center, (x, y, w, h)))\n",
    "            self.detected_once = True\n",
    "        return self.humans\n",
    "\n",
    "    def PredictMotion(self, frame):\n",
    "        for i, (center, box) in enumerate(self.humans):\n",
    "            if i >= MAX_HUMANS_TO_TRACK:\n",
    "                break\n",
    "            prev_center = self.prev_centers.get(i)\n",
    "            if prev_center:\n",
    "                prev_center = tuple(map(int, prev_center))\n",
    "                predicted_coords = self.kfObjs[i].Estimate(*prev_center)\n",
    "                predicted_center = (int(predicted_coords[0]), int(predicted_coords[1]))\n",
    "                cv2.circle(frame, predicted_center, 20, [0, 255, 255], 2, 8)\n",
    "                cv2.putText(frame, \"Predicted\", (predicted_center[0] + 50, predicted_center[1] + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0, 255, 255], 2)\n",
    "            self.prev_centers[i] = center\n",
    "            cv2.circle(frame, center, 20, [0, 0, 255], 2, 8)\n",
    "            cv2.putText(frame, \"Actual\", (center[0] + 50, center[1] + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0, 0, 255], 2)\n",
    "        return frame\n",
    "\n",
    "def main():\n",
    "    processImg = ProcessImage()\n",
    "    vid = cv2.VideoCapture('fast.mp4')\n",
    "    if not vid.isOpened():\n",
    "        print('Cannot open input video')\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        humans = processImg.DetectHumans(frame)\n",
    "        frame = processImg.PredictMotion(frame)\n",
    "\n",
    "        cv2.imshow('Human Detection', frame)\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print('Program Completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc22c6-6990-4d7d-b36d-a61f92855aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
